# Neural networks
Video•
. Duration: 6 minutes
6 min

URL: https://www.coursera.org/learn/uol-how-computers-work/lecture/YSOLq/neural-networks

## VIDEO TRANSCRIPT ## You may navigate through the transcript using tab. To save a note for a section of text press CTRL + S. To expand your selection you may use CTRL + arrow key. You may contract your selection using shift + CTRL + arrow key. For screen readers that are incompatible with using arrow keys for shortcuts, you can replace them with the H J K L keys. Some screen readers may require using CTRL in conjunction with the alt key How do your image recognition models work so well if pixels are such a bad set of features in machine learning? It's because they have a really good feature extraction that can calculate really good features from the original pixels. In fact, these features are learned from data. How does that work? Let's have a look. Features are just numbers. Imagine we have two features for pets, for example, tail length and ear pointiness. These are two numbers, and we can create a new feature, but by combining the two original features together. How do you combine two numbers together to make a new number? There are lots of ways of doing it, but a really simple way is to add them together. So, we have a new feature, which is tail length plus ear pointiness. This isn't necessarily a very good feature. For example, ear pointiness might be much more important for telling if a pet is a cat or a dog than tail length, but they both contribute equally to the sum. We can make ear pointiness more important by multiplying it by a big number and multiplying tail length by a small number. In this new version, they both still have an effect, but ear pointiness is more important. The two numbers we multiply [inaudible] weight. We can actually learn a good feature of this type by using optimization to calculate the best weights. Now, this is a really simple example of a feature. So, you may think that a state-of-the-art machine learning system would need something much more complex. Well, actually, no. Deep neural networks, the cutting-edge machine learning method that has solved many complex AI challenges, works almost exactly like this. How can something so simple solve such hard problems? The basic answer is that these really simple calculations can scale up well when it comes to massive data. The first way this happens is that you don't start with just two basic features, you can have a lot more, maybe hundreds or even thousands of numbers that you add together. Secondly, you can take the newly calculated features and calculate new features from them. The sum we just saw is called a neuron. It is one unit of neural network. But you can have hundreds of neurons. The output of each neuron is fed into our inputs of hundreds of other neurons. The word deep in deep learning means that you have many, many layers of neurons feeding into other neurons. This means that even though the basic calculations are very simple, a deep neural network can learn very complex features. There's one important detail that neural network experts would catch me on. Simply feeding the output of one neuron into another is actually equivalent to simple neurons with different weights, so it doesn't allow you to calculate any more complex features. You need to transform the output by a mathematical function called a nonlinearity. There are many types of nonlinearities, some of them quite complex, but they don't have to be. One very common nonlinearity is just to take any output that's negative and set it to zero. So, the simplest type of neuron just multiplies its input features by weight, adds them up, and if the output is negative, sets this to zero. This simple network can learn very complex features by choosing good weights. I want to be clear here, this is a simple neuron, but that doesn't mean that real neural networks have to use more complex neurons. The one I've described is absolutely state-of-the-art and is used in almost all the best neural network systems. Other types are used for other purposes or types of data, but that doesn't mean they're better. Of course, this is a very simple description of a neural network, and I have left out lots of details. In particular, the learning algorithms, the optimizations used to calculate the weights can be very complex. But the key thing I want you to understand is that a lot of learning algorithms are based on quite simple mathematics, but they work because these calculations can scale up to thousands of features in neurons. The fact that neural networks can be very large and complex helps them work effectively, but it also means that it can be hard for us to understand what they're doing. It's very difficult to interpret the many simple calculations and get a big picture of how neural networks make decisions. This is a problem with many machine learning algorithms because they're very complex and they're learned, not designed by people. It's hard to know how they work. This can make it hard to test and debug models that don't work. It also raises issues when machine learning algorithms make important decisions without us understanding why they make those decisions, something we'll find out more about later. Since we're working with images, I want to talk about another type of neuron that works well for images called a convolutional neural. When I first talked about feature extraction, I talked about how a good way of calculating features from images is to use a filter of the sort you might have in a photo editing application like Photoshop. For example, edge detection and blurring make good features. A convolutional neural is basically a Photoshop filter that transforms an image, but unlike filters in an application, the details of filter can be learned by optimization. The mathematics is quite complicated, but you don't need to know the math to use a convolutional network. Because it's a neural network, the output of one filter can be fed into other filters to create complex new features, which are then themselves fed into other filters to create even more complex features. This is the basic technology that allows machine learning to recognize images so well. ## END TRANSCRIPT ## ## ADDITIONAL PAGE CONTENT ## Lesson 18.1 Data features Discussion Prompt: Data features . Duration: 10 minutes 10 min Video: Video Data features . Duration: 3 minutes 3 min Practice Assignment: Practice quiz – Bag of words . Duration: 30 minutes 30 min Video: Video Neural networks . Duration: 6 minutes 6 min Video: Video Interview: Data features . Duration: 11 minutes 11 min Reading: Reading Artificial neural networks . Duration: 55 minutes 55 min Lesson 18.2 Summary
# Interview: Advice for your first data science project
Videoâ€¢
. Duration: 10 minutes
10 min

URL: https://www.coursera.org/learn/uol-how-computers-work/lecture/ZNvoT/interview-advice-for-your-first-data-science-project

## VIDEO TRANSCRIPT ## You may navigate through the transcript using tab. To save a note for a section of text press CTRL + S. To expand your selection you may use CTRL + arrow key. You may contract your selection using shift + CTRL + arrow key. For screen readers that are incompatible with using arrow keys for shortcuts, you can replace them with the H J K L keys. Some screen readers may require using CTRL in conjunction with the alt key As you're starting to work on your projects, I'm sure you're looking for some advice and some help about how to go about it. So I thought I'd ask a panel of experts who've all had many, many years of experience of working Machine Learning or say teaching Machine Learning about if they have an advice for you. So to start with you Larisa, our students are beginning their first Machine Learning Project creating some image classifiers. What would your advice be to them? My advice would be you get your data first. You cannot do Machine Learning if you don't have data. If you think you have data, it maybe still not good enough data. So you need enough, you need all suitable quality. So until you have it in place, you just cannot be sure that your project will succeed, especially if you use some dataset available somewhere, just make sure you download it, you check it, you fully understand what is inside, how it was gathered, so make sure that are no biases or minimize it. Focus first of all on your data, then you can go further. By the way, it's quite well known that data gathering, data prep-processing will take about 80 percent of your time of all projects. So this is the key. Jamie, does that? Yeah. Absolutely all of that. Eighty-five percent, 90 percent even, you spent a lot of time cleaning data and images as one thing certainly but time series stuff it's like the cleaning of the data is just absolutely everything. It takes so much time and effort and I guess what would tie with that is if you are creating features, plot them, visualize them, have an indication that you can visually see what your data looks like because that really massively helps things and particularly if you're doing classification, you should be able to plot your features in such a way where you can clearly see a difference. Because if you can see a difference, there's a good chance, the algorithm should be able to distinguish the different classes. But definitely plot everything beforehand. Clean it, plot it. Be sure of what you're doing and have an expectation of what you will get at the end. Try and have an idea of what you expect to get so that when you get the result you've got something to compare to or against. Yeah. I mean, from working for you for quite a while, I've realized that you really put a lot of importance on understanding the data before you do anything with it. Yeah. I mean, we can be blind about it and throw it into a bag deep learning algorithm and hope for the best but quite unsatisfactory. You want to understand something, I think sometimes. I mean, I would answer that, the data is key. One of the good rules of thumb to be thinking about is if you anticipate some kind of variation that you want to be able to handle with your classifier in the real-world that have been trained, make sure that variation is there in your training data. Seems obvious but often times people forget about this. But also, I think taking what you know about evaluation metrics and really stepping back and thinking about, okay does cross-validation accuracy match up with what I really want out of this classifier, for example. Sometimes, that is true, right? Sometimes the accuracy of every class is about as important as the accuracy of every other class and you have about the same numbers of each classes in your training data in which case great, that's pretty easy. But if you said you know what? It's really important to me that these particular classes never get confused with each other but if these data like this that's an edge case and as a person I might have a hard time really assessing what class they should go in, I don't care so much about what the accuracy looks like on those. That can be good to take into account when you're really doing that evaluation in the end, whether it's cross validation on your training data or looking at accuracy and how it's broken down in the test data because in the real-world, often we have lots of different priorities and it's not just about getting every type of example as accurate as everything else. There might be all other complicated stuff going on. The other thing that I would stress which I talked to my students about a lot is that no matter how much you know about Machine Learning algorithms, no matter how much you know about your data set, no matter how much you know about your features, if you have a PhD in Machine Learning, you still may not have the intuition to know exactly how to build that Machine Learning system right the first time. So experimentation is key. Learning about the data not just once but trying different things and continuing to try to inspect what's happening at all stages of the Machine Learning pipeline, it's a core part of applied Machine Learning practice. As I mentioned, even people with PhDs in Machine Learning do this, and they get really good at doing it. So don't worry if you don't know what exactly to do because many other people might not know either. They're just exposing the worlds. Often don't know they do. Can I just add to that? If you see that you get like 99 percent accuracy or 99.99 percent accuracy and is looking really good, just know there's problem somewhere. Very, very real is that as a result. Good advice I'll tell that problem to those that if it feels good, then you got to be skeptical. So maybe we can look at the other sides. I mean, obviously never happens to us but what happens when you gather the data, you train the algorithm and it doesn't do what you wanted to, you don't get any accuracy. I mean, any advice for learners in that situation. Alcohol. You should try different algorithms. You analyze your pipelines. So maybe you're overlooking something. Just imagine the situation if for humans have tried to do and if it doesn't work at all. So maybe is that our problems what you're trying to do just really will not work. So you need to rethink the whole design. It does happen. But it's better to plan from the beginning. So what you are doing, try to estimate is likely to work or not. Even if it has 99 percent accuracy, you still need to compare it was performance of other algorithm and if it's completely random, then yes something is wrong at some stage. So try to find that out and see what you can change. Jamie, does this combats your obsession about understanding the data as well? Absolutely. I mean, if for example when you were doing your exploration of your data and you can see some clear differences visually in your representation of your features or visualization your features and your algorithm was not able to pick them up, I think there's still hope. You can go back, find new features, find new ways of representing it. Maybe you're making some mistakes. Do some iterative applications of different algorithms and different features because there's still hope. But if you could never see any difference from the outset, then perhaps that data just the problem isn't solvable using that data. Go back and rethink the whole problem, perhaps, record again. I mean, the path to success is littered with failures and it's just one of those things. Yeah. I mean, that's actually the echoes upon my, I've had a couple of times where you know there's a difference, you know there was an original difference but when you actually look at the features you are using, you can't see that difference, it disappears. So I think there is issues of understanding the feature representation, understanding the data and maybe there's just something wrong in that whole pipeline, the features, the algorithm that you can't spot it. Okay, any other advice? Advice that I give to my student who are doing project is to try to make a clear plan. So it sounds boring but it's so important that you're quit clear where you are in your plan, and when you actually have to finish cleaning your data and start experimenting and if you're not there yet, then what can you spit out? Because if you don't manage your plan, there is huge dangers if you will not finish on time. So most of projects failed or were not as successful as they could because they ran out of time. Know when to stop because if you're at 97 percent accuracy and it looks like it's accurate and genuine, you don't need to keep plugging away. You're okay. I mean, you look at the context of the problem. Machine Learning is never perfect. It's never perfect. Okay, well thanks lots to all of you. Thanks lots for that's fantastic advice. I'm sure our learners will really appreciate and I'm sure you'll join me in saying a big good luck to our learners both in their current projects and hopefully their future lies working with Machine Learning. ## END TRANSCRIPT ## ## ADDITIONAL PAGE CONTENT ## Lesson 20.1 Your data science project Practice Assignment: Preparing for your machine learning project . Duration: 15 minutes 15 min Video: Video Collecting a dataset . Duration: 3 minutes 3 min Discussion Prompt: Advice on data collection . Duration: 30 minutes 30 min Video: Video Interview: Advice for your first data science project . Duration: 10 minutes 10 min Reading: Reading Collecting a dataset . Duration: 15 minutes 15 min Ungraded Plugin: Training a model using your dataset . Duration: 3 hours 3h Practice Assignment: Reflecting on your project . Duration: 30 minutes 30 min Discussion Prompt: Your machine learning project . Duration: 1 hour 1h Lesson 20.2 Summary